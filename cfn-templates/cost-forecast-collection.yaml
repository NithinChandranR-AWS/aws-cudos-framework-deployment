AWSTemplateFormatVersion: '2010-09-09'
Description: 'AWS Cost Explorer Forecast Data Collection - Cloud Intelligence Dashboards'

Parameters:
  ScheduleExpression:
    Type: String
    Default: 'cron(0 1 * * ? *)'
    Description: Schedule expression for when to run the forecast data collection (default is daily at 1:00 AM UTC)
  
  ForecastMonths:
    Type: Number
    Default: 12
    Description: Number of months to forecast (default is 12)
    MinValue: 1
    MaxValue: 12
  
  ServiceGrouping:
    Type: String
    Default: 'true'
    AllowedValues: 
      - 'true'
      - 'false'
    Description: Whether to group forecast data by service (default is true)
  
  Granularity:
    Type: String
    Default: 'MONTHLY'
    AllowedValues:
      - 'MONTHLY'
      - 'DAILY'
    Description: Granularity of forecast data (MONTHLY or DAILY)
  
  ConfidenceLevel:
    Type: Number
    Default: 80
    AllowedValues:
      - 50
      - 75
      - 80
      - 85
      - 90
      - 95
    Description: Confidence level for the forecast prediction interval (default is 80%)
  
  CostMetric:
    Type: String
    Default: 'UNBLENDED_COST'
    AllowedValues:
      - 'UNBLENDED_COST'
      - 'BLENDED_COST'
      - 'AMORTIZED_COST'
      - 'NET_UNBLENDED_COST'
      - 'NET_AMORTIZED_COST'
    Description: Cost metric to use for forecasting
  
  ForecastBucket:
    Type: String
    Default: ''
    Description: >
      S3 bucket to store forecast data. If left empty, the default bucket 'cid-data-{account-id}' will be used.
      To use an existing bucket, make sure the Lambda execution role has write access to it.

  ForecastPrefix:
    Type: String
    Default: 'cost-explorer-forecast'
    Description: S3 prefix for forecast data files
    
  CidDatabaseName:
    Type: String
    Default: 'cid_cur'
    Description: Name of the Athena database where cost data is stored

Resources:
  ForecastDataBucket:
    Type: AWS::S3::Bucket
    Condition: CreateBucket
    Properties:
      BucketName: !Sub "cid-data-${AWS::AccountId}"
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      AccessControl: BucketOwnerFullControl
      OwnershipControls:
        Rules:
          - ObjectOwnership: BucketOwnerEnforced
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: ExpirationRule
            Status: Enabled
            ExpirationInDays: 455
            Prefix: !Ref ForecastPrefix
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: 'W35'
            reason: "Access logging not required for this data bucket"
          - id: 'W51'
            reason: "No bucket policy needed - restricted via IAM"

  ForecastLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - !Sub 'arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
      Policies:
        - PolicyName: CostExplorerAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'ce:GetCostForecast'
                  - 'ce:GetDimensionValues'
                Resource: '*'
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:PutObject'
                  - 's3:GetObject'
                  - 's3:ListBucket'
                Resource:
                  - !If 
                    - CreateBucket
                    - !Sub 'arn:${AWS::Partition}:s3:::${ForecastDataBucket}/*'
                    - !Sub 'arn:${AWS::Partition}:s3:::${ForecastBucket}/*'
                  - !If
                    - CreateBucket
                    - !Sub 'arn:${AWS::Partition}:s3:::${ForecastDataBucket}'
                    - !Sub 'arn:${AWS::Partition}:s3:::${ForecastBucket}'
        - PolicyName: GlueAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'glue:CreateTable'
                  - 'glue:UpdateTable'
                  - 'glue:GetTable'
                  - 'glue:GetDatabase'
                Resource:
                  - !Sub 'arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:catalog'
                  - !Sub 'arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/${CidDatabaseName}'
                  - !Sub 'arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/${CidDatabaseName}/*'

  ForecastLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'CidCostForecastCollector-${AWS::Region}'
      Description: 'Collects and processes cost forecast data from AWS Cost Explorer'
      Handler: index.handler
      Role: !GetAtt ForecastLambdaExecutionRole.Arn
      Runtime: python3.11
      Timeout: 300
      MemorySize: 256
      Environment:
        Variables:
          FORECAST_BUCKET: !If [CreateBucket, !Ref ForecastDataBucket, !Ref ForecastBucket]
          FORECAST_PREFIX: !Ref ForecastPrefix
          FORECAST_MONTHS: !Ref ForecastMonths
          SERVICE_GROUPING: !Ref ServiceGrouping
          GRANULARITY: !Ref Granularity
          CONFIDENCE_LEVEL: !Ref ConfidenceLevel
          METRIC: !Ref CostMetric
          DATABASE_NAME: !Ref CidDatabaseName
      Code:
        ZipFile: |
          import os
          import json
          import logging
          import datetime
          import boto3
          from botocore.exceptions import ClientError

          # Configure logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          # Configuration from environment variables
          FORECAST_BUCKET = os.environ.get('FORECAST_BUCKET', f"cid-data-{boto3.client('sts').get_caller_identity()['Account']}")
          FORECAST_PREFIX = os.environ.get('FORECAST_PREFIX', 'cost-explorer-forecast')
          FORECAST_MONTHS = int(os.environ.get('FORECAST_MONTHS', '12'))
          SERVICE_GROUPING = os.environ.get('SERVICE_GROUPING', 'true').lower() == 'true'
          GRANULARITY = os.environ.get('GRANULARITY', 'MONTHLY')
          CONFIDENCE_LEVEL = int(os.environ.get('CONFIDENCE_LEVEL', '80'))
          METRIC = os.environ.get('METRIC', 'UNBLENDED_COST')
          DATABASE_NAME = os.environ.get('DATABASE_NAME', 'cid_cur')
          TIMESTAMP = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')

          def handler(event, context):
              """Lambda function handler to collect and process AWS cost forecast data"""
              logger.info("Starting cost forecast data collection")
              
              # Ensure the S3 bucket exists
              try:
                  s3_client = boto3.client('s3')
                  s3_client.head_bucket(Bucket=FORECAST_BUCKET)
                  logger.info(f"Using bucket: {FORECAST_BUCKET}")
              except ClientError as e:
                  if e.response['Error']['Code'] == '404':
                      logger.info(f"Bucket {FORECAST_BUCKET} does not exist, creating it...")
                      try:
                          s3_client.create_bucket(Bucket=FORECAST_BUCKET)
                      except Exception as e:
                          logger.error(f"Failed to create bucket {FORECAST_BUCKET}: {str(e)}")
                          raise e
                  else:
                      logger.error(f"Error checking bucket {FORECAST_BUCKET}: {str(e)}")
                      raise e
              
              # Calculate date ranges
              start_date = datetime.date.today().strftime('%Y-%m-01')
              end_date = (datetime.date.today().replace(day=1) + datetime.timedelta(days=32*FORECAST_MONTHS)).replace(day=1).strftime('%Y-%m-01')
              logger.info(f"Forecast period: {start_date} to {end_date}")
              
              # Collect overall forecast data (no grouping)
              overall_forecast = collect_forecast(None, start_date, end_date)
              
              # Collect service-grouped forecast data if enabled
              service_forecast = None
              if SERVICE_GROUPING:
                  service_forecast = collect_forecast('SERVICE', start_date, end_date)
              
              # Upload the data to S3
              if overall_forecast:
                  upload_to_s3(overall_forecast, f"{FORECAST_PREFIX}/overall/{TIMESTAMP}.json")
                  create_athena_partitioned_data(overall_forecast, "ALL", "overall")
              
              if service_forecast:
                  upload_to_s3(service_forecast, f"{FORECAST_PREFIX}/by-service/{TIMESTAMP}.json")
                  create_athena_partitioned_data(service_forecast, "SERVICE", "by-service")
              
              # Create a latest pointer file
              latest_pointer = {
                  "timestamp": TIMESTAMP,
                  "path": f"s3://{FORECAST_BUCKET}/{FORECAST_PREFIX}/overall/{TIMESTAMP}.json"
              }
              upload_to_s3(latest_pointer, f"{FORECAST_PREFIX}/latest.json")
              
              logger.info("Cost forecast data collection completed successfully")
              return {
                  'statusCode': 200,
                  'body': json.dumps({
                      'message': 'Cost forecast data collection completed successfully',
                      'timestamp': TIMESTAMP,
                      'dataLocation': f"s3://{FORECAST_BUCKET}/{FORECAST_PREFIX}/"
                  })
              }

          def collect_forecast(group_by, start_date, end_date):
              """Collect forecast data from AWS Cost Explorer"""
              ce_client = boto3.client('ce')
              
              try:
                  logger.info(f"Collecting forecast data {'with grouping by ' + group_by if group_by else 'without grouping'}")
                  
                  kwargs = {
                      'TimePeriod': {
                          'Start': start_date,
                          'End': end_date
                      },
                      'Metric': METRIC,
                      'Granularity': GRANULARITY,
                      'PredictionIntervalLevel': CONFIDENCE_LEVEL
                  }
                  
                  if group_by:
                      kwargs['GroupBy'] = [{'Type': 'DIMENSION', 'Key': group_by}]
                  
                  response = ce_client.get_cost_forecast(**kwargs)
                  logger.info(f"Successfully collected forecast data")
                  return response
              except Exception as e:
                  logger.error(f"Failed to collect forecast data: {str(e)}")
                  return None

          def upload_to_s3(data, key):
              """Upload data to S3"""
              try:
                  s3_client = boto3.client('s3')
                  s3_client.put_object(
                      Bucket=FORECAST_BUCKET,
                      Key=key,
                      Body=json.dumps(data, default=str),
                      ContentType='application/json'
                  )
                  logger.info(f"Successfully uploaded data to s3://{FORECAST_BUCKET}/{key}")
              except Exception as e:
                  logger.error(f"Failed to upload data to S3: {str(e)}")
                  raise e

          def create_athena_partitioned_data(forecast_data, dimension_type, data_type):
              """Create Athena-friendly partitioned data"""
              try:
                  year_month = datetime.date.today().strftime('%Y-%m')
                  partition_path = f"{FORECAST_PREFIX}/partitioned/year={year_month[:4]}/month={year_month[5:7]}"
                  
                  # Process data based on whether it has groups or not
                  processed_data = []
                  
                  if dimension_type == "ALL":
                      for item in forecast_data.get('ForecastResultsByTime', []):
                          record = {
                              "dimension": dimension_type,
                              "value": "ALL",
                              "metric": METRIC,
                              "startdate": item['TimePeriod']['Start'],
                              "enddate": item['TimePeriod']['End'],
                              "meanvalue": item['MeanValue'],
                              "lowerbound": item.get('PredictionIntervalLowerBound', 0),
                              "upperbound": item.get('PredictionIntervalUpperBound', 0)
                          }
                          processed_data.append(record)
                  else:
                      for item in forecast_data.get('ForecastResultsByTime', []):
                          for group in item.get('Groups', []):
                              record = {
                                  "dimension": dimension_type,
                                  "value": group['Key'],
                                  "metric": METRIC,
                                  "startdate": item['TimePeriod']['Start'],
                                  "enddate": item['TimePeriod']['End'],
                                  "meanvalue": group['Metrics'][METRIC]['Amount'],
                                  "lowerbound": group['Metrics'][METRIC].get('PredictionIntervalLowerBound', 0),
                                  "upperbound": group['Metrics'][METRIC].get('PredictionIntervalUpperBound', 0)
                              }
                              processed_data.append(record)
                  
                  # Upload the processed data to S3
                  s3_client = boto3.client('s3')
                  s3_client.put_object(
                      Bucket=FORECAST_BUCKET,
                      Key=f"{partition_path}/{data_type}.json",
                      Body=json.dumps(processed_data, default=str),
                      ContentType='application/json'
                  )
                  
                  # Ensure Athena table exists
                  ensure_athena_table_exists()
                  
                  logger.info(f"Successfully created Athena-friendly data for {dimension_type}")
              except Exception as e:
                  logger.error(f"Failed to create Athena-friendly data: {str(e)}")
                  raise e

          def ensure_athena_table_exists():
              """Ensure the Athena table for cost forecast data exists"""
              try:
                  glue_client = boto3.client('glue')
                  table_name = 'cost_forecast_data'
                  
                  try:
                      # Check if table exists
                      glue_client.get_table(
                          DatabaseName=DATABASE_NAME,
                          Name=table_name
                      )
                      logger.info(f"Table {table_name} already exists in database {DATABASE_NAME}")
                  except glue_client.exceptions.EntityNotFoundException:
                      # Create the table if it doesn't exist
                      logger.info(f"Creating table {table_name} in database {DATABASE_NAME}")
                      
                      # Define the table
                      table_input = {
                          'Name': table_name,
                          'StorageDescriptor': {
                              'Columns': [
                                  {'Name': 'dimension', 'Type': 'string'},
                                  {'Name': 'value', 'Type': 'string'},
                                  {'Name': 'metric', 'Type': 'string'},
                                  {'Name': 'startdate', 'Type': 'string'},
                                  {'Name': 'enddate', 'Type': 'string'},
                                  {'Name': 'meanvalue', 'Type': 'double'},
                                  {'Name': 'lowerbound', 'Type': 'double'},
                                  {'Name': 'upperbound', 'Type': 'double'}
                              ],
                              'Location': f's3://{FORECAST_BUCKET}/{FORECAST_PREFIX}/partitioned',
                              'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',
                              'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',
                              'SerdeInfo': {
                                  'SerializationLibrary': 'org.openx.data.jsonserde.JsonSerDe'
                              }
                          },
                          'PartitionKeys': [
                              {'Name': 'year', 'Type': 'string'},
                              {'Name': 'month', 'Type': 'string'}
                          ],
                          'TableType': 'EXTERNAL_TABLE',
                          'Parameters': {
                              'classification': 'json',
                              'updated_by': 'CID Cost Forecast Lambda',
                              'created_at': datetime.datetime.now().isoformat()
                          }
                      }
                      
                      glue_client.create_table(
                          DatabaseName=DATABASE_NAME,
                          TableInput=table_input
                      )
                      
                      logger.info(f"Successfully created table {table_name} in database {DATABASE_NAME}")
              except Exception as e:
                  logger.error(f"Error ensuring Athena table exists: {str(e)}")
                  # Continue execution even if table creation fails
                  pass

  EventRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub 'CidCostForecastSchedule-${AWS::Region}'
      Description: 'Schedule for cost forecast data collection'
      ScheduleExpression: !Ref ScheduleExpression
      State: ENABLED
      Targets:
        - Id: ForecastLambdaFunction
          Arn: !GetAtt ForecastLambdaFunction.Arn

  EventRulePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt ForecastLambdaFunction.Arn
      Action: 'lambda:InvokeFunction'
      Principal: 'events.amazonaws.com'
      SourceArn: !GetAtt EventRule.Arn

Conditions:
  CreateBucket: !Equals [!Ref ForecastBucket, '']
  
Outputs:
  ForecastDataBucket:
    Description: 'The S3 bucket where forecast data is stored'
    Value: !If [CreateBucket, !Ref ForecastDataBucket, !Ref ForecastBucket]
  
  ForecastDataLocation:
    Description: 'The S3 path where forecast data is stored'
    Value:
      Fn::Join:
        - ''
        - - 's3://'
          - !If [CreateBucket, !Ref ForecastDataBucket, !Ref ForecastBucket]
          - '/'
          - !Ref ForecastPrefix
          - '/'
    
  LambdaFunction:
    Description: 'The Lambda function that collects cost forecast data'
    Value: !Ref ForecastLambdaFunction
    
  ScheduleExpression:
    Description: 'The schedule expression for when the forecast data collection runs'
    Value: !Ref ScheduleExpression
